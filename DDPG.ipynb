{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DDPG.ipynb","provenance":[{"file_id":"1cdzf-cFCUoRPM8yjaaVZTlt-gRQUzmLk","timestamp":1643192385928}],"authorship_tag":"ABX9TyPe4QCP4T3tNMoMc0w9UuWY"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eEMgJ_oHllp0","executionInfo":{"status":"ok","timestamp":1643335893428,"user_tz":-540,"elapsed":5696,"user":{"displayName":"양현준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLCzvni_KcFKU5U-vmQ2H1e1aU5225oHgsDxya=s64","userId":"03791106996878442638"}},"outputId":"c11bb79e-a5e7-410c-86e1-8aca8d81ebc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: gym in /usr/local/lib/python3.7/dist-packages (0.17.3)\n","Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.7/dist-packages (from gym) (1.19.5)\n","Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.5.0)\n","Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /usr/local/lib/python3.7/dist-packages (from gym) (1.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gym) (1.4.1)\n","Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from pyglet<=1.5.0,>=1.4.0->gym) (0.16.0)\n"]}],"source":["! pip install gym"]},{"cell_type":"code","source":["import numpy as np\n","import torch \n","import torch.nn as nn\n","import gym\n","import random\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","from torch.distributions.categorical import Categorical"],"metadata":{"id":"djX0cwNEs5sh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["env = gym.make(\"Pendulum-v0\")\n","env"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6CoTHtIRuUO8","executionInfo":{"status":"ok","timestamp":1643335900324,"user_tz":-540,"elapsed":15,"user":{"displayName":"양현준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLCzvni_KcFKU5U-vmQ2H1e1aU5225oHgsDxya=s64","userId":"03791106996878442638"}},"outputId":"c411d807-128e-49a4-b8c9-4ec7a372bcdb"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<TimeLimit<PendulumEnv<Pendulum-v0>>>"]},"metadata":{},"execution_count":3}]},{"cell_type":"code","source":["#Pendulum action space\n","env.action_space"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"gVgbVeoB2dmx","executionInfo":{"status":"ok","timestamp":1643335900324,"user_tz":-540,"elapsed":13,"user":{"displayName":"양현준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLCzvni_KcFKU5U-vmQ2H1e1aU5225oHgsDxya=s64","userId":"03791106996878442638"}},"outputId":"74528c08-9f91-4cb3-9f3d-87bd6b4cb157"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(-2.0, 2.0, (1,), float32)"]},"metadata":{},"execution_count":4}]},{"cell_type":"code","source":["#Pendulum state space \n","env.observation_space"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QB3Z6jZM2rUt","executionInfo":{"status":"ok","timestamp":1643335900324,"user_tz":-540,"elapsed":11,"user":{"displayName":"양현준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLCzvni_KcFKU5U-vmQ2H1e1aU5225oHgsDxya=s64","userId":"03791106996878442638"}},"outputId":"72529de6-19bd-49ea-fb9b-246bf079fd94"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Box(-8.0, 8.0, (3,), float32)"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["#(코사인, 사인, 각속도)\n","env.reset()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zqOL0kBR2-_t","executionInfo":{"status":"ok","timestamp":1643335900325,"user_tz":-540,"elapsed":10,"user":{"displayName":"양현준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLCzvni_KcFKU5U-vmQ2H1e1aU5225oHgsDxya=s64","userId":"03791106996878442638"}},"outputId":"6e1c3f9b-b64c-4f5b-fa18-477a2dc03e2e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([-0.09803687,  0.99518278,  0.35303924])"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["class OUProcess:\n","  def __init__(self, mu):\n","      self.theta, self.dt, self.sigma = 0.1, 0.01, 0.1\n","      self.mu = mu\n","      self.x_prev = np.zeros_like(self.mu)\n","\n","  def __call__(self):\n","      x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n","          self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n","      self.x_prev = x\n","      return x"],"metadata":{"id":"IzZPA7ZMo6Fu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Experience Replay (deque 대신)\n","class ReplayMemory:\n","  def __init__(self, max_size):\n","    self.buffer = [None]*max_size\n","    self.max_size = max_size\n","    self.index = 0\n","    self.size = 0\n","\n","  def push(self, obj):\n","    self.buffer[self.index] = obj\n","    self.size = min(self.size+1, self.max_size)\n","    #max_size 넘어가면 다시 인덱스 = 0 \n","    self.index = (self.index+1) % self.max_size \n","  \n","  def sample(self, batch_size):\n","    #배치 사이즈만큼 랜덤하게 인덱스 추출\n","    indices = random.sample(range(self.size), batch_size) \n","    return [self.buffer[index] for index in indices]\n","\n","  def __len__(self):\n","    return self.size"],"metadata":{"id":"85xl6iWhlkMN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def prepare_training_inputs(sampled_exps, device=\"cpu\"):\n","  states = []\n","  actions = []\n","  rewards = []\n","  next_states = []\n","  dones = []\n","\n","  for sampled_exp in sampled_exps:\n","    states.append(sampled_exp[0])\n","    actions.append(sampled_exp[1])\n","    rewards.append(sampled_exp[2])\n","    next_states.append(sampled_exp[3])\n","    dones.append(sampled_exp[4])\n","  \n","  states = torch.cat(states, dim=0).float().to(device)\n","  actions = torch.cat(actions, dim=0).to(device)\n","  rewards = torch.cat(rewards, dim=0).float().to(device)\n","  next_states = torch.cat(next_states, dim=0).float().to(device)\n","  dones = torch.cat(dones, dim=0).float().to(device)\n","  return states, actions, rewards, next_states, dones"],"metadata":{"id":"Ot7rnygstsGJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MLP(nn.Module):\n","  def __init__(self, \n","               input_dim:int, output_dim:int,\n","               hidden_act:str, out_act: str):\n","\n","    super().__init__()\n","    self.input_dim = input_dim\n","    self.output_dim = output_dim\n","    self.hidden_act = getattr(nn, hidden_act)()\n","    self.out_act = getattr(nn, out_act)()\n","\n","    self.layers = nn.ModuleList()\n","    self.layers.append(nn.Linear(self.input_dim, 128))\n","    self.layers.append(self.hidden_act)\n","    self.layers.append(nn.Linear(128, 64))\n","    self.layers.append(self.hidden_act)\n","    self.layers.append(nn.Linear(64, self.output_dim))\n","    self.layers.append(self.out_act)\n","\n","  def forward(self, xs):\n","    for layer in self.layers:\n","      xs = layer(xs)\n","    return xs"],"metadata":{"id":"vLbT1TAe41Fb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Actor, Critic\n","class Actor(nn.Module):\n","  def __init__(self):\n","    super(Actor, self).__init__()\n","\n","    self.mlp = MLP(input_dim=3,\n","                   output_dim=1,\n","                   hidden_act=\"ReLU\",\n","                   out_act=\"Identity\")\n","    \n","  def forward(self, state):\n","    return self.mlp(state).clamp(-2.0, 2.0)\n","  \n","class Critic(nn.Module):\n","  def __init__(self):\n","    super(Critic, self).__init__()\n","\n","    self.q_estimator = MLP(input_dim=128,\n","                           output_dim=1,\n","                           hidden_act=\"ReLU\",\n","                           out_act=\"Identity\")\n","\n","  def forward(self, s, a):\n","    s_vec = nn.Linear(in_features=3, out_features=64)(s)\n","    s_vec = nn.ReLU()(s_vec)\n","    \n","    a_vec = nn.Linear(in_features=1, out_features=64)(a)\n","    a_vec = nn.ReLU()(a_vec)\n","      \n","    emb = torch.cat([s_vec, a_vec], dim=-1)\n","    return self.q_estimator(emb)"],"metadata":{"id":"je2Dljkt3PvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class DDPG(nn.Module):\n","  def __init__(self,\n","               critic:nn.Module,\n","               critic_target:nn.Module,\n","               actor:nn.Module,\n","               actor_target:nn.Module,\n","               lr_critic:float = 0.0005,\n","               lr_actor:float = 0.001,\n","               gamma:float = 0.99):\n","    super().__init__()\n","\n","    self.critic = critic\n","    self.actor = actor\n","    self.lr_critic = lr_critic\n","    self.lr_actor = lr_actor\n","    self.gamma = gamma\n","\n","    #optimizer\n","    self.critic_opt = torch.optim.Adam(params=critic.parameters(), lr=lr_critic)\n","    self.actor_opt = torch.optim.Adam(params=actor.parameters(), lr=lr_actor)\n","\n","    #target network\n","    critic_target.load_state_dict(critic.state_dict())\n","    self.critic_target = critic_target\n","    actor_target.load_state_dict(actor.state_dict())\n","    self.actor_target = actor_target\n","\n","    self.criteria = nn.SmoothL1Loss()\n","  \n","  def get_action(self, state):\n","    with torch.no_grad():\n","      a = self.actor(state)\n","    return a\n","  \n","  def update(self, state, action, reward, next_state, done):\n","    s, a, r, ns = state, action, reward, next_state\n","\n","    with torch.no_grad():\n","      target = r + self.gamma*self.critic_target(ns, self.actor_target(ns))*(1-done)\n","    critic_loss = self.criteria(self.critic(s, a), target)\n","\n","    self.critic_opt.zero_grad()\n","    critic_loss.backward()\n","    self.critic_opt.step()\n","\n","    actor_loss = -self.critic(s, self.actor(s)).mean()\n","    self.actor_opt.zero_grad()\n","    actor_loss.backward()\n","    self.actor_opt.step()"],"metadata":{"id":"8cJgaVikF0jZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Soft target update\n","def soft_update(net, net_target, tau):\n","  for param_target, param in zip(net_target.parameters(), net.parameters()):\n","    param_target.data.copy_(param_target.data*(1.0-tau) + param.data*tau)"],"metadata":{"id":"BfYh8XNqj7_I"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lr_actor = 0.005\n","lr_critic = 0.001\n","gamma = 0.99\n","batch_size = 256\n","memory_size = 50000\n","tau = 0.001\n","sampling_only_until = 2000 "],"metadata":{"id":"15ag1ZcCn5x9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["actor, actor_target = Actor(), Actor()\n","critic, critic_target = Critic(), Critic()\n","\n","agent = DDPG(critic=critic,\n","             critic_target=critic_target,\n","             actor=actor,\n","             actor_target=actor_target)\n","\n","memory = ReplayMemory(max_size=memory_size)"],"metadata":{"id":"zvinLsAvlCrZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_eps = 200\n","print_every = 10\n","\n","for n_epi in range(total_eps):\n","  ou_noise = OUProcess(mu=np.zeros(1))\n","  s = env.reset()\n","  cum_r = 0\n","\n","  while True:\n","    s = torch.tensor(s).float().view(1,3)\n","    a = np.array(agent.get_action(s)) + ou_noise()[0]\n","    ns, r, done, info = env.step(a)\n","\n","    experience = (s, \n","                  torch.tensor(a).view(1,1),\n","                  torch.tensor(r).view(1,1),\n","                  torch.tensor(ns).float().view(1,3),\n","                  torch.tensor(done).view(1,1))\n","    \n","    memory.push(experience)\n","\n","    s = ns\n","    cum_r += r\n","\n","    if len(memory) >= sampling_only_until:\n","      #train agent\n","      sampled_exps = memory.sample(batch_size)\n","      sampled_exps = prepare_training_inputs(sampled_exps)\n","      agent.update(*sampled_exps)\n","      #update target network\n","      soft_update(agent.actor, agent.actor_target, tau)\n","      soft_update(agent.critic, agent.critic_target, tau)\n","\n","    if done:\n","      break\n","    \n","  if n_epi % print_every == 0:\n","    msg = (n_epi, cum_r) \n","    print(\"Episode: {} | Cumulative Reward: {}\".format(*msg))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n_UQScNSoUwS","executionInfo":{"status":"ok","timestamp":1643350658469,"user_tz":-540,"elapsed":367682,"user":{"displayName":"양현준","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiLCzvni_KcFKU5U-vmQ2H1e1aU5225oHgsDxya=s64","userId":"03791106996878442638"}},"outputId":"ce845c0d-fa63-473d-ca89-ffac5135ac50"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Episode: 0 | Cumulative Reward: [-1249.9844]\n","Episode: 10 | Cumulative Reward: [-1671.9213]\n","Episode: 20 | Cumulative Reward: [-862.99005]\n","Episode: 30 | Cumulative Reward: [-1395.553]\n","Episode: 40 | Cumulative Reward: [-1081.5059]\n","Episode: 50 | Cumulative Reward: [-1434.7241]\n","Episode: 60 | Cumulative Reward: [-861.8643]\n","Episode: 70 | Cumulative Reward: [-1078.3242]\n","Episode: 80 | Cumulative Reward: [-1452.9554]\n","Episode: 90 | Cumulative Reward: [-670.442]\n","Episode: 100 | Cumulative Reward: [-1117.2136]\n","Episode: 110 | Cumulative Reward: [-1069.6384]\n","Episode: 120 | Cumulative Reward: [-969.6323]\n","Episode: 130 | Cumulative Reward: [-1319.126]\n","Episode: 140 | Cumulative Reward: [-1236.5948]\n","Episode: 150 | Cumulative Reward: [-1058.4697]\n","Episode: 160 | Cumulative Reward: [-1017.4551]\n","Episode: 170 | Cumulative Reward: [-1882.1885]\n","Episode: 180 | Cumulative Reward: [-1513.8538]\n","Episode: 190 | Cumulative Reward: [-1251.8711]\n"]}]}]}